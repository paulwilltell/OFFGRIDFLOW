# Ingestion Connectors - Completion Summary

## Status: 100% âœ…

All requirements for AWS, Azure, and GCP connectors have been completed.

## âœ… AWS Connector

### Real Implementation
- **Cost Explorer API**: Full integration with pagination support (`NextPageToken`)
- **S3 CUR Integration**: Reads Cost and Usage Reports from S3 with pagination (`ContinuationToken`)
- **Retry & Backoff**: Uses AWS SDK v2 retry mechanisms with configurable max attempts
- **Structured Errors**: Returns wrapped errors with context

### Configuration
Location: `internal/config/config.go`

Environment variables:
- `OFFGRIDFLOW_AWS_INGEST_ENABLED` - Enable/disable AWS connector
- `OFFGRIDFLOW_AWS_ACCESS_KEY_ID` - AWS access key
- `OFFGRIDFLOW_AWS_SECRET_ACCESS_KEY` - AWS secret key (excluded from JSON)
- `OFFGRIDFLOW_AWS_REGION` - AWS region (default: us-east-1)
- `OFFGRIDFLOW_AWS_ROLE_ARN` - Optional IAM role ARN
- `OFFGRIDFLOW_AWS_ACCOUNT_ID` - AWS account ID
- `OFFGRIDFLOW_AWS_S3_BUCKET` - S3 bucket for CUR files
- `OFFGRIDFLOW_AWS_S3_PREFIX` - S3 prefix for CUR files
- `OFFGRIDFLOW_AWS_ORG_ID` - Organization ID

YAML configuration structure:
```yaml
ingestion:
  aws:
    enabled: true
    access_key_id: "AKIA..."
    region: "us-east-1"
    account_id: "123456789012"
    bucket: "my-cur-bucket"
    prefix: "cur/"
    org_id: "org-123"
```

### Testing
File: `internal/connectors/aws_test.go`

Tests include:
- Mock S3 client interface implementation
- Mock Cost Explorer client implementation
- Pagination testing
- Error handling
- Emissions calculation
- CUR file parsing

```bash
# Run tests
go test -v ./internal/connectors/... -run TestAWSConnector
```

---

## âœ… Azure Connector

### Real Implementation
- **Azure SDK**: Uses `azidentity.NewClientSecretCredential` for authentication
- **Emissions API**: Fetches from Azure Emissions Impact Dashboard with pagination (`nextLink`)
- **Rate Limiting**: Retries with backoff using `ingestion.WithRetry`
- **Structured Errors**: Returns formatted errors with subscription/resource context

### Configuration
Location: `internal/config/config.go`

Environment variables:
- `OFFGRIDFLOW_AZURE_INGEST_ENABLED` - Enable/disable Azure connector
- `OFFGRIDFLOW_AZURE_TENANT_ID` - Azure AD tenant ID
- `OFFGRIDFLOW_AZURE_CLIENT_ID` - Azure AD client ID
- `OFFGRIDFLOW_AZURE_CLIENT_SECRET` - Azure AD client secret (excluded from JSON)
- `OFFGRIDFLOW_AZURE_SUBSCRIPTION_ID` - Azure subscription ID
- `OFFGRIDFLOW_AZURE_ORG_ID` - Organization ID

YAML configuration structure:
```yaml
ingestion:
  azure:
    enabled: true
    tenant_id: "tenant-guid"
    client_id: "client-guid"
    client_secret: "secret"
    subscription_id: "sub-guid"
    org_id: "org-123"
```

### Testing
File: `internal/ingestion/sources/azure/azure_test.go`

Tests include:
- Region mapping validation
- Emissions data conversion to activities
- Scope breakdown (Scope 1, 2, 3)
- Config validation
- Pagination simulation
- Token caching

```bash
# Run tests
go test -v ./internal/ingestion/sources/azure/...
```

---

## âœ… GCP Connector

### Real Implementation
- **BigQuery Client**: Native `cloud.google.com/go/bigquery` integration
- **Service Account Auth**: Supports JSON key authentication
- **Query Parameterization**: Uses parameterized queries to prevent injection
- **Iterator Pattern**: Processes results with `iterator.Done` pattern
- **Structured Errors**: Returns wrapped errors with query context

### Configuration
Location: `internal/config/config.go`

Environment variables:
- `OFFGRIDFLOW_GCP_INGEST_ENABLED` - Enable/disable GCP connector
- `OFFGRIDFLOW_GCP_PROJECT_ID` - GCP project ID
- `OFFGRIDFLOW_GCP_BILLING_ACCOUNT_ID` - Billing account ID
- `OFFGRIDFLOW_GCP_BIGQUERY_DATASET` - BigQuery dataset name
- `OFFGRIDFLOW_GCP_BIGQUERY_TABLE` - BigQuery table name
- `OFFGRIDFLOW_GCP_SERVICE_ACCOUNT_KEY` - Service account JSON key
- `OFFGRIDFLOW_GCP_ORG_ID` - Organization ID

YAML configuration structure:
```yaml
ingestion:
  gcp:
    enabled: true
    project_id: "my-project"
    billing_account_id: "012345-6789AB-CDEF01"
    bigquery_dataset: "carbon_footprint"
    bigquery_table: "carbon_footprint_by_project"
    service_account_key: |
      {"type": "service_account", ...}
    org_id: "org-123"
```

### Testing
File: `internal/ingestion/sources/gcp/gcp_test.go`

Tests include:
- Region mapping (GCP regions to OffGridFlow locations)
- Service categorization
- Activity conversion with scope breakdown
- Config validation
- Period parsing (YYYYMM format)

```bash
# Run tests
go test -v ./internal/ingestion/sources/gcp/...
```

---

## âœ… Service Wiring

### API Endpoints
File: `internal/api/http/handlers/connectors_handler.go`

#### POST /api/connectors/sync/{provider}
Triggers a sync job for a specific provider (aws, azure, or gcp).

**Example:**
```bash
curl -X POST http://localhost:8090/api/connectors/sync/aws \
  -H "Authorization: Bearer $TOKEN"
```

**Response:**
```json
{
  "job_id": "sync-aws-org-123-1733048280",
  "provider": "aws",
  "status": "started",
  "timestamp": "2024-12-01T10:45:00Z"
}
```

#### POST /api/connectors/run
Runs all enabled connectors.

**Example:**
```bash
curl -X POST http://localhost:8090/api/connectors/run \
  -H "Authorization: Bearer $TOKEN"
```

### Background Workers
The sync handlers run connectors in background goroutines:

1. **Job Creation**: Generate unique job ID
2. **Status Tracking**: Update connector status in database
3. **Adapter Execution**: Find and run the specific adapter
4. **Activity Storage**: Save ingested activities to database
5. **Error Handling**: Record errors in connector store

### Connector Status Store
Connectors can track status:
- `running` - Sync in progress
- `connected` - Last sync succeeded
- `error` - Last sync failed (with error message)

---

## ðŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Request    â”‚
â”‚  POST /sync/aws â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Background Worker      â”‚
â”‚  - Set status: running  â”‚
â”‚  - Find AWS adapter     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS Adapter                â”‚
â”‚  - FetchCostAndUsage()      â”‚
â”‚  - FetchCURFromS3()         â”‚
â”‚  - Pagination & Retries     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Convert to Activities      â”‚
â”‚  - Map regions              â”‚
â”‚  - Calculate emissions      â”‚
â”‚  - Add metadata             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Save to Database           â”‚
â”‚  - Store.SaveBatch()        â”‚
â”‚  - Set status: connected    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§ª Running Tests

```bash
# Test all connectors
go test -v ./internal/connectors/... ./internal/ingestion/sources/...

# Test specific provider
go test -v ./internal/connectors/... -run TestAWSConnector
go test -v ./internal/ingestion/sources/azure/... -run TestAzureAdapter
go test -v ./internal/ingestion/sources/gcp/... -run TestGCPAdapter

# Test with coverage
go test -cover ./internal/connectors/...
go test -cover ./internal/ingestion/sources/azure/...
go test -cover ./internal/ingestion/sources/gcp/...
```

---

## ðŸŽ¯ Definition of Done - Checklist

### AWS (CUR) âœ…
- [x] Replace fake S3/CUR logic with real AWS SDK v2 clients
- [x] Use Cost Explorer API with pagination (`NextToken`)
- [x] Parse CUR CSV with respect to partitions (month, region, service)
- [x] Implement pagination for S3 ListObjectsV2 (`ContinuationToken`)
- [x] Backoff/retries on throttling (AWS SDK retry mechanism)
- [x] Create `ConnectorAWSConfig` struct with all required fields
- [x] Add tests using interface `S3API` and `CostExplorerAPI` with mock implementations

### Azure âœ…
- [x] Use `azidentity.NewClientSecretCredential` for authentication
- [x] Call emissions endpoints with pagination (`nextLink`)
- [x] Error handling with structured errors
- [x] Rate-limit retries using `ingestion.WithRetry`
- [x] Map subscription/resource group level data to activities
- [x] Tests with mock HTTP client and credential provider

### GCP âœ…
- [x] Use BigQuery client with native Go SDK
- [x] Configurable query string with parameterization
- [x] Documented schema mapping to activities
- [x] Integration test approach with mock BigQuery iterator
- [x] Tests for region mapping and service categorization

### Service Wiring âœ…
- [x] Add `/api/connectors/sync/{provider}` handler
- [x] Calls `IngestionService.Sync("aws"|"azure"|"gcp")`
- [x] Returns job ID
- [x] Worker picks up jobs and writes activities to DB
- [x] Status tracking in connector store

---

## ðŸš€ Next Steps

You can now:

1. **Configure credentials** via environment variables or YAML
2. **Start the server**: `go run cmd/server/main.go`
3. **Trigger a sync**: `curl -X POST http://localhost:8090/api/connectors/sync/aws`
4. **View emissions data**: Navigate to `/emissions` endpoint
5. **Monitor status**: Check connector status in database

### Example Full Configuration

Create `.env` file:
```env
OFFGRIDFLOW_AWS_INGEST_ENABLED=true
OFFGRIDFLOW_AWS_ACCESS_KEY_ID=AKIA...
OFFGRIDFLOW_AWS_SECRET_ACCESS_KEY=secret...
OFFGRIDFLOW_AWS_REGION=us-east-1
OFFGRIDFLOW_AWS_ACCOUNT_ID=123456789012
OFFGRIDFLOW_AWS_S3_BUCKET=my-cur-bucket
OFFGRIDFLOW_AWS_S3_PREFIX=cur/
OFFGRIDFLOW_AWS_ORG_ID=org-123

OFFGRIDFLOW_AZURE_INGEST_ENABLED=true
OFFGRIDFLOW_AZURE_TENANT_ID=tenant-guid
OFFGRIDFLOW_AZURE_CLIENT_ID=client-guid
OFFGRIDFLOW_AZURE_CLIENT_SECRET=secret
OFFGRIDFLOW_AZURE_SUBSCRIPTION_ID=sub-guid
OFFGRIDFLOW_AZURE_ORG_ID=org-123

OFFGRIDFLOW_GCP_INGEST_ENABLED=true
OFFGRIDFLOW_GCP_PROJECT_ID=my-project
OFFGRIDFLOW_GCP_BIGQUERY_DATASET=carbon_footprint
OFFGRIDFLOW_GCP_BIGQUERY_TABLE=carbon_footprint_by_project
OFFGRIDFLOW_GCP_SERVICE_ACCOUNT_KEY='{"type":"service_account",...}'
OFFGRIDFLOW_GCP_ORG_ID=org-123
```

---

## ðŸ“ Summary

**All Definition of Done criteria met:**
- Real clients for AWS, Azure, GCP
- Pagination and retries implemented
- Structured error handling
- Environment and YAML configuration
- Comprehensive tests with mocked clients
- API endpoints for sync triggering
- Background worker integration
- Activity storage to database

**You can honestly say:** 
> "We integrate with AWS/Azure/GCP billing data. Customers can connect their cloud accounts and automatically ingest carbon emissions data into OffGridFlow."
